{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PQqK0G7_M_q"
      },
      "source": [
        "## Performance Measures\n",
        "\n",
        "There are many different ways to measure the correctness of our models, such as raw accuracy or span accuracy. However, since this dataset contains morpheme *allomorphy*, we are simplifying our approach and calculating the f1 score of the morphemes generated, treating each set of morphemes as a bag of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iRvZim44_pK2"
      },
      "outputs": [],
      "source": [
        "def evaluate(gold, pred):\n",
        "\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "\n",
        "  for g, p in zip(gold, pred):\n",
        "    g_bag = g.strip().split(\" \")\n",
        "    p_bag = p.strip().split(\" \")\n",
        "\n",
        "    tp += sum([1 for i in p_bag if i in g_bag])\n",
        "    fp += sum([1 for i in p_bag if not i in g_bag])\n",
        "    fn += sum([1 for i in g_bag if not i in p_bag])\n",
        "\n",
        "  precision = tp / (tp + fp)\n",
        "  recall = tp / (tp + fn)\n",
        "  if precision == 0 or recall == 0:\n",
        "    f1 = 0\n",
        "  else:\n",
        "    f1 = 2 / ((1/precision) + (1/recall))\n",
        "\n",
        "  return {\n",
        "      \"f1\": f1,\n",
        "      \"precision\": precision,\n",
        "      \"recall\": recall,\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBGrnPyKDyJb"
      },
      "source": [
        "## Unigram SentencePiece\n",
        "\n",
        "First, we need to install and import [sentencepiece](https://github.com/google/sentencepiece). This unsupervised tokenization toolkit includes both byte-pair-encoding (BPE) and unigram based algorithms, but we will use the unigram method as out baseline here because of its superior performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbDQvzcKDOUi",
        "outputId": "0d594b15-be3a-49d6-dad7-457c5bde8154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\jingy\\appdata\\roaming\\python\\python311\\site-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-SYTrjpq2o_s"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I26sCEF8XUB"
      },
      "source": [
        "Now, we will train sentencpiece on our train file for the language of interest -- we do not need to provide the segmented target files since this is an unsupervised technique. Vocab size is chosen manually for each language based on hyper-parameter search. For Shipibo-Konibo (shp) I recommend a vocab size of __316__. For Rar√°muri/Tahumara (tar) I recommend a vocab size of __412__.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XRoOt_Pv3G0-"
      },
      "outputs": [],
      "source": [
        "lang = \"tar\"\n",
        "vocab_size=412\n",
        "\n",
        "train_file = f'miniproj1-dataset/{lang}.train.src'\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=train_file,\n",
        "    model_prefix=f'unigram_{lang}',\n",
        "    vocab_size=vocab_size,\n",
        "    model_type='unigram'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCkb9-4s3aa2",
        "outputId": "55750842-a0c4-49ef-8424-60dd4ab9f5ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'f1': 0.3181818181818182, 'precision': 0.26013513513513514, 'recall': 0.4095744680851064}\n"
          ]
        }
      ],
      "source": [
        "input_file = f'miniproj1-dataset/{lang}.dev.src'\n",
        "output_file = f'miniproj1-dataset/{lang}.dev.tgt'\n",
        "\n",
        "s = spm.SentencePieceProcessor(model_file=f'unigram_{lang}.model')\n",
        "\n",
        "golds = []\n",
        "preds = []\n",
        "\n",
        "for word, morph in zip(open(input_file), open(output_file)):\n",
        "  gold = morph.strip()\n",
        "  pred = ' '.join(s.encode(word, out_type=str))[1:].lstrip()\n",
        "  golds.append(gold)\n",
        "  preds.append(pred)\n",
        "\n",
        "print(evaluate(golds, preds))\n",
        "\n",
        "with open(f'unigram_{lang}.dev.tgt', 'w') as f:\n",
        "  f.write(\"\\n\".join(preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbNm-g2v9kod"
      },
      "source": [
        "## Morfessor\n",
        "\n",
        "First, we install and and import [Morfessor 2.0](https://morfessor.readthedocs.io/en/latest/). Like Sentencepiece, this algorithm uses unlabelled data in order to generate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rej-Mx_73y84",
        "outputId": "e497b25b-82a5-4cae-c0fb-6cffc445bbe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: morfessor in c:\\users\\jingy\\appdata\\roaming\\python\\python311\\site-packages (2.0.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install morfessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V9Ydfkl49_lI"
      },
      "outputs": [],
      "source": [
        "import morfessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A256673bLfTu"
      },
      "source": [
        "Because the algorithm is unsupervised, we only need to provide the src file. Unlike Sentencepiece, there is no need to specify the vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRAwDuMU-N1x",
        "outputId": "c40abec9-eec9-42f8-a5b7-cdc856f17355"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "............."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "...........................................\n",
            "........................................................\n",
            "........................................................\n",
            "........................................................\n",
            "........................................................\n",
            "........................................................\n",
            "........................................................\n",
            "........................................................\n",
            "........................................................\n",
            "........................................................\n",
            "........................................................\n"
          ]
        }
      ],
      "source": [
        "lang = \"tar\"\n",
        "\n",
        "train_file = f'miniproj1-dataset/{lang}.train.src'\n",
        "\n",
        "io = morfessor.MorfessorIO()\n",
        "data = list(io.read_corpus_file(train_file))\n",
        "model = morfessor.BaselineModel()\n",
        "model.load_data(data)\n",
        "model.train_batch()\n",
        "io.write_binary_file(f'morfessor_{lang}.model', model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvun2aC3Jm4l",
        "outputId": "e98c061b-7a9c-49d8-d70d-7df8a0ed5f46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'f1': 0.29708222811671087, 'precision': 0.2962962962962963, 'recall': 0.2978723404255319}\n"
          ]
        }
      ],
      "source": [
        "input_file = f'miniproj1-dataset/{lang}.dev.src'\n",
        "output_file = f'miniproj1-dataset/{lang}.dev.tgt'\n",
        "\n",
        "model = io.read_binary_model_file(f'morfessor_{lang}.model')\n",
        "\n",
        "golds = []\n",
        "preds = []\n",
        "\n",
        "for word, morph in zip(open(input_file), open(output_file)):\n",
        "  gold = morph.strip()\n",
        "  pred = \" \".join(model.viterbi_segment(word)[0]).strip()\n",
        "  golds.append(gold)\n",
        "  preds.append(pred)\n",
        "\n",
        "print(evaluate(golds, preds))\n",
        "\n",
        "with open(f'morfessor_{lang}.dev.tgt', 'w') as f:\n",
        "  f.write(\"\\n\".join(preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jciBYmIePTba"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
